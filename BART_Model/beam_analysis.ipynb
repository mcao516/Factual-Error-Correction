{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rouge\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_results(p, r, f):\n",
    "    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)\n",
    "\n",
    "avg_evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                            max_n=4,\n",
    "                            limit_length=True,\n",
    "                            length_limit=200,\n",
    "                            length_limit_type='words',\n",
    "                            apply_avg=True,\n",
    "                            apply_best=False,\n",
    "                            alpha=0.5, # Default F1_score\n",
    "                            weight_factor=1.2,\n",
    "                            stemming=True)\n",
    "\n",
    "best_evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                             max_n=4,\n",
    "                             limit_length=True,\n",
    "                             length_limit=200,\n",
    "                             length_limit_type='words',\n",
    "                             apply_avg=False,\n",
    "                             apply_best=True,\n",
    "                             alpha=0.5, # Default F1_score\n",
    "                             weight_factor=1.2,\n",
    "                             stemming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN/DailyMail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnndm_preds = 'preds/cnndm_test_bm4_all.hypo.tokenized'\n",
    "cnndm_target = 'preds/test.target.tokenzied'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = read_lines(cnndm_preds)\n",
    "target = read_lines(cnndm_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45960\n",
      "11490\n"
     ]
    }
   ],
   "source": [
    "print(len(preds))\n",
    "print((len(target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_size = 4\n",
    "preds = [preds[i::beam_size] for i in range(beam_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(preds[0]) == len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- top 1:\n",
      "\trouge-1:\tP: 40.41\tR: 51.69\tF1: 44.29\n",
      "\trouge-2:\tP: 19.34\tR: 24.69\tF1: 21.16\n",
      "\trouge-3:\tP: 11.69\tR: 14.89\tF1: 12.77\n",
      "\trouge-4:\tP:  8.02\tR: 10.19\tF1:  8.74\n",
      "\trouge-l:\tP: 34.11\tR: 42.16\tF1: 37.08\n",
      "\trouge-w:\tP: 22.07\tR: 13.21\tF1: 16.02\n",
      "- top 2:\n",
      "\trouge-1:\tP: 41.37\tR: 50.52\tF1: 44.41\n",
      "\trouge-2:\tP: 19.84\tR: 24.14\tF1: 21.25\n",
      "\trouge-3:\tP: 12.00\tR: 14.54\tF1: 12.82\n",
      "\trouge-4:\tP:  8.24\tR:  9.94\tF1:  8.77\n",
      "\trouge-l:\tP: 34.87\tR: 41.45\tF1: 37.23\n",
      "\trouge-w:\tP: 22.72\tR: 12.98\tF1: 16.01\n",
      "- top 3:\n",
      "\trouge-1:\tP: 42.41\tR: 49.10\tF1: 44.41\n",
      "\trouge-2:\tP: 20.41\tR: 23.52\tF1: 21.31\n",
      "\trouge-3:\tP: 12.39\tR: 14.21\tF1: 12.89\n",
      "\trouge-4:\tP:  8.53\tR:  9.73\tF1:  8.84\n",
      "\trouge-l:\tP: 35.75\tR: 40.62\tF1: 37.38\n",
      "\trouge-w:\tP: 23.51\tR: 12.72\tF1: 16.00\n",
      "- top 4:\n",
      "\trouge-1:\tP: 43.62\tR: 47.39\tF1: 44.31\n",
      "\trouge-2:\tP: 21.00\tR: 22.67\tF1: 21.24\n",
      "\trouge-3:\tP: 12.74\tR: 13.66\tF1: 12.83\n",
      "\trouge-4:\tP:  8.75\tR:  9.32\tF1:  8.77\n",
      "\trouge-l:\tP: 36.70\tR: 39.53\tF1: 37.39\n",
      "\trouge-w:\tP: 24.32\tR: 12.36\tF1: 15.88\n"
     ]
    }
   ],
   "source": [
    "for i, beam_pred in enumerate(preds):\n",
    "    print('- No. {}:'.format(i + 1))\n",
    "    scores = avg_evaluator.get_scores(beam_pred, target)\n",
    "    for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "        print(prepare_results(results['p'], results['r'], results['f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11490/11490 [07:31<00:00, 25.44it/s]\n"
     ]
    }
   ],
   "source": [
    "index, best_predictions = 0, []\n",
    "for t in tqdm(target):\n",
    "    best_rouge_1, best_pred = -1.0, None\n",
    "    for s in range(beam_size):\n",
    "        p = preds[s][index]\n",
    "        score = avg_evaluator.get_scores([p], [t])\n",
    "        if score['rouge-1']['f'] > best_rouge_1:\n",
    "            best_rouge_1 = score['rouge-1']['f']\n",
    "            best_pred = p\n",
    "    best_predictions.append(best_pred)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\trouge-1:\tP: 44.82\tR: 52.40\tF1: 47.25\n",
      "\trouge-2:\tP: 22.21\tR: 25.79\tF1: 23.33\n",
      "\trouge-3:\tP: 13.71\tR: 15.80\tF1: 14.34\n",
      "\trouge-4:\tP:  9.53\tR: 10.91\tF1:  9.93\n",
      "\trouge-l:\tP: 37.35\tR: 42.75\tF1: 39.24\n",
      "\trouge-w:\tP: 24.77\tR: 13.50\tF1: 16.98\n"
     ]
    }
   ],
   "source": [
    "scores = avg_evaluator.get_scores(best_predictions, target)\n",
    "for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "    print(prepare_results(results['p'], results['r'], results['f']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_preds_path = 'preds/xsum_test_bm6_all.hypo.tokenized'\n",
    "xsum_target_path = 'preds/test_xsum.target.tokenzied'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsum_preds = read_lines(xsum_preds_path)\n",
    "xsum_target = read_lines(xsum_target_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67806\n",
      "11301\n"
     ]
    }
   ],
   "source": [
    "print(len(xsum_preds))\n",
    "print(len(xsum_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_size = 6\n",
    "xsum_preds = [xsum_preds[i::beam_size] for i in range(beam_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(xsum_preds[0]) == len(xsum_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- top 1:\n",
      "\trouge-1:\tP: 41.50\tR: 49.71\tF1: 44.58\n",
      "\trouge-2:\tP: 19.55\tR: 23.54\tF1: 21.04\n",
      "\trouge-3:\tP: 10.91\tR: 13.20\tF1: 11.76\n",
      "\trouge-4:\tP:  6.57\tR:  8.00\tF1:  7.08\n",
      "\trouge-l:\tP: 39.06\tR: 45.47\tF1: 41.58\n",
      "\trouge-w:\tP: 27.78\tR: 18.28\tF1: 21.63\n",
      "- top 2:\n",
      "\trouge-1:\tP: 42.29\tR: 49.24\tF1: 44.82\n",
      "\trouge-2:\tP: 19.93\tR: 23.28\tF1: 21.14\n",
      "\trouge-3:\tP: 11.11\tR: 13.02\tF1: 11.79\n",
      "\trouge-4:\tP:  6.69\tR:  7.88\tF1:  7.11\n",
      "\trouge-l:\tP: 39.72\tR: 45.16\tF1: 41.81\n",
      "\trouge-w:\tP: 28.37\tR: 18.14\tF1: 21.70\n",
      "- top 3:\n",
      "\trouge-1:\tP: 42.96\tR: 48.49\tF1: 44.86\n",
      "\trouge-2:\tP: 20.24\tR: 22.92\tF1: 21.16\n",
      "\trouge-3:\tP: 11.22\tR: 12.76\tF1: 11.75\n",
      "\trouge-4:\tP:  6.70\tR:  7.66\tF1:  7.02\n",
      "\trouge-l:\tP: 40.32\tR: 44.66\tF1: 41.92\n",
      "\trouge-w:\tP: 28.90\tR: 17.91\tF1: 21.70\n",
      "- top 4:\n",
      "\trouge-1:\tP: 43.91\tR: 47.77\tF1: 45.02\n",
      "\trouge-2:\tP: 20.82\tR: 22.68\tF1: 21.36\n",
      "\trouge-3:\tP: 11.62\tR: 12.68\tF1: 11.92\n",
      "\trouge-4:\tP:  7.00\tR:  7.66\tF1:  7.18\n",
      "\trouge-l:\tP: 41.24\tR: 44.27\tF1: 42.21\n",
      "\trouge-w:\tP: 29.75\tR: 17.76\tF1: 21.81\n",
      "- top 5:\n",
      "\trouge-1:\tP: 45.10\tR: 46.82\tF1: 45.17\n",
      "\trouge-2:\tP: 21.42\tR: 22.22\tF1: 21.44\n",
      "\trouge-3:\tP: 11.98\tR: 12.42\tF1: 11.98\n",
      "\trouge-4:\tP:  7.24\tR:  7.51\tF1:  7.23\n",
      "\trouge-l:\tP: 42.29\tR: 43.65\tF1: 42.44\n",
      "\trouge-w:\tP: 30.73\tR: 17.49\tF1: 21.85\n",
      "- top 6:\n",
      "\trouge-1:\tP: 46.55\tR: 45.56\tF1: 45.22\n",
      "\trouge-2:\tP: 22.34\tR: 21.82\tF1: 21.67\n",
      "\trouge-3:\tP: 12.53\tR: 12.23\tF1: 12.14\n",
      "\trouge-4:\tP:  7.51\tR:  7.33\tF1:  7.26\n",
      "\trouge-l:\tP: 43.60\tR: 42.80\tF1: 42.64\n",
      "\trouge-w:\tP: 31.96\tR: 17.13\tF1: 21.86\n"
     ]
    }
   ],
   "source": [
    "for i, p in enumerate(xsum_preds):\n",
    "    print('- top {}:'.format(i + 1))\n",
    "    scores = avg_evaluator.get_scores(p, xsum_target)\n",
    "    for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "        print(prepare_results(results['p'], results['r'], results['f']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11301/11301 [03:28<00:00, 54.08it/s]\n"
     ]
    }
   ],
   "source": [
    "index, best_predictions = 0, []\n",
    "for t in tqdm(xsum_target):\n",
    "    best_rouge_1, best_pred = -1.0, None\n",
    "    for s in range(beam_size):\n",
    "        p = xsum_preds[s][index]\n",
    "        score = avg_evaluator.get_scores([p], [t])\n",
    "        if score['rouge-1']['f'] > best_rouge_1:\n",
    "            best_rouge_1 = score['rouge-1']['f']\n",
    "            best_pred = p\n",
    "    best_predictions.append(best_pred)\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\trouge-1:\tP: 49.15\tR: 51.97\tF1: 49.76\n",
      "\trouge-2:\tP: 24.57\tR: 25.77\tF1: 24.77\n",
      "\trouge-3:\tP: 14.44\tR: 15.04\tF1: 14.50\n",
      "\trouge-4:\tP:  9.09\tR:  9.43\tF1:  9.10\n",
      "\trouge-l:\tP: 45.10\tR: 47.21\tF1: 45.63\n",
      "\trouge-w:\tP: 33.19\tR: 19.19\tF1: 23.89\n"
     ]
    }
   ],
   "source": [
    "scores = avg_evaluator.get_scores(best_predictions, xsum_target)\n",
    "for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "    print(prepare_results(results['p'], results['r'], results['f']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
